{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "In the previous exercise, you implemented feedforward propagation for neural networks and used it to predict handwritten digits with the weights we provided. In this exercise, you will implement the backpropagation algorithm to learn the parameters for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('data/ex3data1.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 400), (5000, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the simoid function yet again;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model representation\n",
    "\n",
    "Our neural network is shown in Figure 2. It has 3 layers {an input layer, a hidden layer and an output layer. Recall that our inputs are pixel values of digit images. Since the images are of size 20 x 20, this gives us 400 input layer units (not counting the extra bias unit which always outputs +1). \n",
    "\n",
    "The training data will be loaded into the variables $X$ and $y$.\n",
    "\n",
    "You have been provided with a set of network parameters already trained by us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward and cost function\n",
    "\n",
    "Taken from the end of exercise 3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(a1, theta):\n",
    "    m = a1.shape[0]\n",
    "    \n",
    "    a1 = np.insert(a1, 0, values=np.ones(m), axis=1)\n",
    "\n",
    "    # Do the first Linear step \n",
    "    z2 = a1.dot(theta[0].T)\n",
    "\n",
    "    # Put it through the first activation function\n",
    "    a2 = sigmoid(z2)\n",
    "    a2 = np.insert(a2, 0, values=np.ones(m), axis=1)\n",
    "    # Second linear step\n",
    "    z3 = a2.dot(theta[1].T)\n",
    "    \n",
    "    # Put through second activation function\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    return a1, z2, a2, z3, a3\n",
    "\n",
    "    m = X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, learning_rate):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    theta1 = nn_params[:hidden_layer_size*(input_layer_size+1)]\n",
    "    theta1 = theta1.reshape(hidden_layer_size, (input_layer_size+1), order='F')\n",
    "    theta2 = nn_params[hidden_layer_size*(input_layer_size+1):]\n",
    "    theta2 = theta2.reshape(num_labels, (hidden_layer_size+1), order='F')\n",
    "    theta = (theta1, theta2)\n",
    "    \n",
    "    a1, z2, a2, z3, a3 = forward_prop(X, theta)\n",
    "    \n",
    "    J = 0\n",
    "    for i in range(m):\n",
    "        inside = np.multiply(-y[i,:], np.log(a3[i,:])) - np.multiply((1 - y[i,:]), np.log(1 - a3[i,:]))\n",
    "        J += (1/m) * np.sum(inside)\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, learning_rate):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Reconstruct the parameters theta1 and theta2\n",
    "    theta1 = nn_params[:hidden_layer_size*(input_layer_size+1)]\n",
    "    theta1 = theta1.reshape(hidden_layer_size, (input_layer_size+1), order='F')\n",
    "    theta2 = nn_params[hidden_layer_size*(input_layer_size+1):]\n",
    "    theta2 = theta2.reshape(num_labels, (hidden_layer_size+1), order='F')\n",
    "    \n",
    "    theta = (theta1, theta2)\n",
    "\n",
    "    a1, z2, a2, z3, a3 = forward_prop(X, theta)\n",
    "    \n",
    "    J = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        inside = np.multiply(-y[i,:], np.log(a3[i,:])) - np.multiply((1 - y[i,:]), np.log(1 - a3[i,:]))\n",
    "        J += (1/m) * np.sum(inside)\n",
    "    \n",
    "    #reg term\n",
    "    J += learning_rate/(2*m)*(np.sum(np.square(theta1[:, 1:])) + np.sum(np.square(theta2[:, 1:])))\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_initial(input_size, output_size, num_labels, epsilon_initial):\n",
    "    params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - epsilon_initial) * epsilon_initial\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was having a tough time getting the parameters to load correctly, so I've skipped some of preliminary testing. I might come back to this, but if the end results are correct, I probably won't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "In this part of the exercise, you will implement the backpropagation algorithm to compute the gradient for the neural network cost function.\n",
    "\n",
    "You will first implement the backpropagation algorithm to compute the gradients for the parameters for the (unregularized) neural network. After you have verified that your gradient computation for the unregularized case is correct, you will implement the gradient for the regularized neural network.\n",
    "\n",
    "Now, you will implement the backpropagation algorithm. Recall that the intuition behind the backpropagation algorithm is as follows. Given a training example (x(t); y(t)), we will first run a \"forward pass\" to compute all the activations throughout the network, including the output value of the hypothesis Then, for each node $j$ in layer $l$, we would like to compute an \"error term\" that measures how much that node was \"responsible\" for any errors in our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_gradient(z):\n",
    "    return np.multiply(sigmoid(z), (1 - sigmoid(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, learning_rate):\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    delta1 = np.zeros([hidden_layer_size, input_layer_size+1])\n",
    "    delta2 = np.zeros([num_labels, hidden_layer_size+1])\n",
    "\n",
    "    theta1 = nn_params[:hidden_layer_size*(input_layer_size+1)]\n",
    "    theta1 = theta1.reshape(hidden_layer_size, (input_layer_size+1), order='F')\n",
    "    \n",
    "    theta2 = nn_params[hidden_layer_size*(input_layer_size+1):]\n",
    "    theta2 = theta2.reshape(num_labels, (hidden_layer_size+1), order='F')\n",
    "    theta = (theta1, theta2)\n",
    "    \n",
    "    a1, z2, a2, z3, a3 = forward_prop(X, theta)\n",
    "\n",
    "    for i in range(m):\n",
    "        \n",
    "        a1_i = a1[i,:]\n",
    "        z2_i = z2[i,:]\n",
    "        a2_i = a2[i,:]\n",
    "        a3_i = a3[i,:]\n",
    "        y_i = y[i,:]\n",
    "       \n",
    "        delta3 = a3_i - y_i\n",
    "             \n",
    "        z2_i = np.insert(z2_i, 0, values=np.ones(1))\n",
    "        \n",
    "        delta2_i = np.multiply((theta2.T * delta3.T).T, sigmoid_gradient(z2_i))\n",
    "        \n",
    "        delta1 = delta1 + (delta2_i[:,1:]).T * a1_i\n",
    "        delta2 = delta2 + delta3.T * a2_i\n",
    "        \n",
    "    theta1_grad = delta1 / m\n",
    "    theta2_grad = delta2 / m\n",
    "    \n",
    "    theta1_grad[:,1:] = theta1_grad[:,1:] + (theta1[:,1:] * learning_rate) / m\n",
    "    theta2_grad[:,1:] = theta2_grad[:,1:] + (theta2[:,1:] * learning_rate) / m\n",
    "\n",
    "    grad = np.concatenate((np.ravel(delta1), np.ravel(delta2)))\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, learning_rate):\n",
    "    grad = gradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, learning_rate)\n",
    "    cost = reg_cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, learning_rate)\n",
    "    return cost, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 401), (10, 26))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initial setup\n",
    "input_size = 400\n",
    "hidden_size = 25\n",
    "num_labels = 10\n",
    "learning_rate = 1\n",
    "\n",
    "# randomly initialize a parameter array of the size of the full network's parameters\n",
    "params = (np.random.random(size=hidden_size * (input_size + 1) + num_labels * (hidden_size + 1)) - 0.5) * 0.25\n",
    "\n",
    "m = X.shape[0]\n",
    "X = (X - X.mean()) / X.std()\n",
    "X = np.matrix(X) \n",
    "y = np.matrix(y)\n",
    "\n",
    "# unravel the parameter array into parameter matrices for each layer\n",
    "theta1 = np.matrix(np.reshape(params[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "theta2 = np.matrix(np.reshape(params[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "\n",
    "theta1.shape, theta2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 400\n",
    "hidden_size = 25\n",
    "num_labels = 10\n",
    "learning_rate = 1\n",
    "epsilon_initial = 0.12\n",
    "\n",
    "params = rand_initial(input_size, hidden_size, num_labels, epsilon_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters using fmincg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 3.6531435405714685\n",
       "     jac: array([-58.55298272,  26.06424678,  26.06424678, ..., 424.42782147,\n",
       "       321.79354602, 219.92256293])\n",
       " message: 'Converged (|f_n-f_(n-1)| ~= 0)'\n",
       "    nfev: 79\n",
       "     nit: 2\n",
       "  status: 1\n",
       " success: True\n",
       "       x: array([ 0.04463695, -0.03343172,  0.03256961, ..., -0.06443979,\n",
       "       -0.00052869, -0.12559057])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmin = minimize(fun=backprop, x0=params, args=(input_size, hidden_size, num_labels, X, y, learning_rate), \n",
    "                method='TNC', jac=True, options={'maxiter': 250})\n",
    "fmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, nn_fit):\n",
    "    \n",
    "    theta1 = np.matrix(np.reshape(nn_fit[:hidden_size * (input_size + 1)], (hidden_size, (input_size + 1))))\n",
    "    theta2 = np.matrix(np.reshape(nn_fit[hidden_size * (input_size + 1):], (num_labels, (hidden_size + 1))))\n",
    "    \n",
    "    theta = (theta1, theta2)\n",
    "    \n",
    "    a1, z2, a2, z3, a3 = forward_prop(X, theta)\n",
    "    \n",
    "    y_pred = np.array(np.argmax(a3, axis=1) + 1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
