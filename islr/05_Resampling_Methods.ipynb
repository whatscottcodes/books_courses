{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 - Resampling Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1** Using basic statistical properties of the variance, as well as single variable calculus, derive (5.6). In other words, prove that Î± given by (5.6) does indeed minimize $Var(\\alpha X + (1-\\alpha)Y)$.\n",
    "\n",
    ">This ended up being more LaTex to type out than I want to commit to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2a** What is the probability that the first bootstrap observation is not the jth observation from the original sample ? Justify your answer.\n",
    "\n",
    "> $1 - \\frac{1}{n}$, just the probability **not** selecting one obseration.\n",
    "\n",
    "**2b** What is the probability that the second bootstrap observation is not the jth observation from the original sample?\n",
    "\n",
    "> It's the same $1 - \\frac{1}{n}$, just the probability **not** selecting one obseration.\n",
    "\n",
    "**2c** Bootstrap samples with replacement so each observation in the sample has the same chance of equalling the jth observation, multiplying the probabilities together we get $(1-\\frac{1}{n})^n$\n",
    "\n",
    "**2d** 67.2%\n",
    "\n",
    "**2e** 63.4%\n",
    "\n",
    "**2f** 63.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6723199999999999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(1-1/5)**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6339676587267709"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(1-1/100)**100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6323045752290363"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-(1-1/1000)**1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 100001)\n",
    "y = 1 - (1 - 1/x) ** x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAFYJJREFUeJzt3X+QXWd93/H3xzKykxSDjDbEsWRL7ghqFzrY3XEgboMbalt4Mpj8mI7UppiU1tM2ZlpI2toDY1MxTGiHSRhSD6C0CoEpKK7JJBpGjOr6R9MpP6L12PiHQPZaNHgRCUtsXCbQCFnf/nGP4Hi9e+9Zaa2Vz3m/Zu7sOc/znHufR2f1uXfPee45qSokScNxxmp3QJJ0ahn8kjQwBr8kDYzBL0kDY/BL0sAY/JI0MAa/JA2MwS9JA2PwS9LAnLnaHVho/fr1tWnTptXuhiS9oNx3333fqqqpLm1Pu+DftGkTMzMzq90NSXpBSfKnXdt6qEeSBsbgl6SBMfglaWAMfkkaGINfkgZmYvAn2ZXkm0keXqI+ST6UZDbJg0kua9Vdn+Sx5nH9SnZcknRiunzi/xiwdUz9G4EtzeMG4MMASc4FbgV+CrgcuDXJupPprCTp5E0M/qr6Y+DJMU2uAz5eI18AXprkPOAa4M6qerKqngLuZPwbyEn57pGj/OZ/P8j9X3vq+XoJSeqFlTjGfz7wRGt9rilbqvw5ktyQZCbJzPz8/Al14ntHnuFDd8/y0NefPqHtJWkoViL4s0hZjSl/bmHVzqqarqrpqalO3ziWJJ2glQj+OWBja30DcHhMuSRpFa1E8O8B3tLM7nkt8HRVfQPYB1ydZF1zUvfqpkyStIomXqQtyaeAK4H1SeYYzdR5EUBVfQTYC1wLzALfBX6lqXsyyXuB/c1T7aiqcSeJJUmnwMTgr6rtE+oL+NUl6nYBu06sa5Kk50Pvvrlbi54+liQd15vgTxabRCRJWqg3wS9J6sbgl6SBMfglaWAMfkkamN4FfzmtR5LG6k3wO6dHkrrpTfBLkrox+CVpYAx+SRoYg1+SBsbgl6SB6V3wO5lTksbrTfB7jTZJ6qY3wS9J6sbgl6SB6RT8SbYmOZhkNslNi9RfmOSuJA8muTfJhlbdM0keaB57VrLzkqTl63LP3TXAbcBVwBywP8meqjrQavYB4ONV9XtJfhb4DeAfN3Xfq6rXrHC/JUknqMsn/suB2ao6VFVHgN3AdQvaXALc1Szfs0i9JOk00SX4zweeaK3PNWVtXwJ+sVn+eeDFSV7WrJ+dZCbJF5K8+aR624EX55Sk8boE/2ITJRfG668Dr09yP/B64OvA0abugqqaBv4h8MEkf/05L5Dc0Lw5zMzPz3fv/bM66XxOSeqiS/DPARtb6xuAw+0GVXW4qn6hqi4F3tWUPX28rvl5CLgXuHThC1TVzqqarqrpqampExmHJKmjLsG/H9iSZHOStcA24Fmzc5KsT3L8uW4GdjXl65KcdbwNcAXQPiksSTrFJgZ/VR0FbgT2AV8Gbq+qR5LsSPKmptmVwMEkjwIvB97XlF8MzCT5EqOTvu9fMBtIknSKTZzOCVBVe4G9C8puaS3fAdyxyHafA159kn2UJK0gv7krSQPTu+B3Nqckjdef4Hc2pyR10p/glyR1YvBL0sAY/JI0MAa/JA1M74K/vEqbJI3Vu+CXJI3Xm+D3ZuuS1E1vgl+S1I3BL0kDY/BL0sAY/JI0MAa/JA1Mb4LfST2S1E1vgl+S1I3BL0kD0yn4k2xNcjDJbJKbFqm/MMldSR5Mcm+SDa2665M81jyuX8nOS5KWb2LwJ1kD3Aa8EbgE2J7kkgXNPgB8vKr+FrAD+I1m23OBW4GfAi4Hbk2ybuW6L0lari6f+C8HZqvqUFUdAXYD1y1ocwlwV7N8T6v+GuDOqnqyqp4C7gS2nny3JUknqkvwnw880Vqfa8ravgT8YrP888CLk7ys47YryotzStJ4XYJ/sZmSC+P114HXJ7kfeD3wdeBox21JckOSmSQz8/PzHbq0SCe9SpskddIl+OeAja31DcDhdoOqOlxVv1BVlwLvasqe7rJt03ZnVU1X1fTU1NQyhyBJWo4uwb8f2JJkc5K1wDZgT7tBkvVJjj/XzcCuZnkfcHWSdc1J3aubMknSKpkY/FV1FLiRUWB/Gbi9qh5JsiPJm5pmVwIHkzwKvBx4X7Ptk8B7Gb157Ad2NGWSpFVyZpdGVbUX2Lug7JbW8h3AHUtsu4sf/gUgSVplfnNXkgamd8Ffz500JElq6U3wO5lTkrrpTfBLkrox+CVpYAx+SRoYg1+SBqZ3we9F2iRpvN4FvyRpvN4EvxfnlKRuehP8kqRuDH5JGhiDX5IGxuCXpIHpXfA7m1OSxutN8MfLtElSJ70JfklSNwa/JA1Mp+BPsjXJwSSzSW5apP6CJPckuT/Jg0mubco3Jflekgeax0dWegCSpOWZeM/dJGuA24CrgDlgf5I9VXWg1ezdjG7C/uEklzC6P++mpu7xqnrNynZbknSiunzivxyYrapDVXUE2A1ct6BNAec0yy8BDq9cFyVJK6lL8J8PPNFan2vK2t4D/HKSOUaf9t/eqtvcHAL6n0n+7sl0tguvzilJ43UJ/sXmSS6M1+3Ax6pqA3At8IkkZwDfAC6oqkuBdwKfTHLOgm1JckOSmSQz8/PzyxvBD57jhDaTpMHpEvxzwMbW+gaeeyjnbcDtAFX1eeBsYH1V/VVV/UVTfh/wOPCKhS9QVTurarqqpqemppY/CklSZ12Cfz+wJcnmJGuBbcCeBW2+BrwBIMnFjIJ/PslUc3KYJBcBW4BDK9V5SdLyTZzVU1VHk9wI7APWALuq6pEkO4CZqtoD/BrwO0newegw0FurqpL8DLAjyVHgGeCfV9WTz9toJEkTTQx+gKray+ikbbvsltbyAeCKRbb7NPDpk+yjJGkF+c1dSRqY3gV/eX1OSRqrd8EvSRrP4JekgTH4JWlgDH5JGhiDX5IGpnfB70XaJGm83gW/JGm83gS/V+eUpG56E/ySpG4MfkkaGINfkgbG4JekgTH4JWlgehP8WfTWwJKkhXoT/JKkbgx+SRqYTsGfZGuSg0lmk9y0SP0FSe5Jcn+SB5Nc26q7udnuYJJrVrLzkqTlm3jP3SRrgNuAq4A5YH+SPc19do97N3B7VX04ySWM7s+7qVneBvxN4CeB/5HkFVX1zEoPRJLUTZdP/JcDs1V1qKqOALuB6xa0KeCcZvklwOFm+Tpgd1X9VVV9FZhtnk+StEq6BP/5wBOt9bmmrO09wC8nmWP0af/ty9h2RZWX55SksboE/2LzJBem63bgY1W1AbgW+ESSMzpuS5IbkswkmZmfn+/QpUU66WxOSeqkS/DPARtb6xv44aGc494G3A5QVZ8HzgbWd9yWqtpZVdNVNT01NdW995KkZesS/PuBLUk2J1nL6GTtngVtvga8ASDJxYyCf75pty3JWUk2A1uAP1mpzkuSlm/irJ6qOprkRmAfsAbYVVWPJNkBzFTVHuDXgN9J8g5Gh3LeWqOD7Y8kuR04ABwFftUZPZK0uiYGP0BV7WV00rZddktr+QBwxRLbvg9430n0UZK0gvzmriQNTO+C39mckjReb4Lf2ZyS1E1vgl+S1I3BL0kDY/BL0sAY/JI0ML0Lfif1SNJ4vQt+SdJ4vQn+eHlOSeqkN8EvSerG4JekgTH4JWlgDH5JGpjeBb8XaZOk8XoT/M7pkaRuehP8kqRuDH5JGphOwZ9ka5KDSWaT3LRI/W8leaB5PJrk2626Z1p1C2/SLkk6xSbeczfJGuA24CpgDtifZE9zn10AquodrfZvBy5tPcX3quo1K9dlSdLJ6PKJ/3JgtqoOVdURYDdw3Zj224FPrUTnJEkrr0vwnw880Vqfa8qeI8mFwGbg7lbx2UlmknwhyZtPuKcdldfnlKSxJh7qYfGZkkul6zbgjqp6plV2QVUdTnIRcHeSh6rq8We9QHIDcAPABRdc0KFLi3TS+ZyS1EmXT/xzwMbW+gbg8BJtt7HgME9VHW5+HgLu5dnH/4+32VlV01U1PTU11aFLkqQT1SX49wNbkmxOspZRuD9ndk6SVwLrgM+3ytYlOatZXg9cARxYuK0k6dSZeKinqo4muRHYB6wBdlXVI0l2ADNVdfxNYDuwu+pZF024GPhokmOM3mTe354NJEk69boc46eq9gJ7F5TdsmD9PYts9zng1SfRP0nSCvObu5I0ML0Lfq/OKUnj9Sb4veeuJHXTm+CXJHVj8EvSwBj8kjQwBr8kDUzvgt9JPZI0Xu+CX5I0nsEvSQNj8EvSwBj8kjQwBr8kDYzBL0kD07/g9yptkjRWr4Lf67RJ0mS9Cn5J0mQGvyQNTKfgT7I1ycEks0luWqT+t5I80DweTfLtVt31SR5rHtevZOclScs38Z67SdYAtwFXAXPA/iR72jdNr6p3tNq/Hbi0WT4XuBWYZnQZnfuabZ9a0VFIkjrr8on/cmC2qg5V1RFgN3DdmPbbgU81y9cAd1bVk03Y3wlsPZkOS5JOTpfgPx94orU+15Q9R5ILgc3A3cvZNskNSWaSzMzPz3fp95KczClJ43UJ/sUmSS6Vr9uAO6rqmeVsW1U7q2q6qqanpqY6dGlxzuaUpMm6BP8csLG1vgE4vETbbfzwMM9yt5UknQJdgn8/sCXJ5iRrGYX7noWNkrwSWAd8vlW8D7g6ybok64CrmzJJ0iqZOKunqo4muZFRYK8BdlXVI0l2ADNVdfxNYDuwu+qH10yoqieTvJfRmwfAjqp6cmWHIElajonBD1BVe4G9C8puWbD+niW23QXsOsH+SZJWmN/claSB6V3we3FOSRqvV8EfL88pSRP1KvglSZMZ/JI0MAa/JA2MwS9JA9O74C8v0yZJY/Uu+CVJ4/Uq+J3MKUmT9Sr4JUmTGfySNDAGvyQNjMEvSQPTu+D3Im2SNF6vgt9rtEnSZL0KfknSZAa/JA1Mp+BPsjXJwSSzSW5aos0/SHIgySNJPtkqfybJA83jOTdplySdWhPvuZtkDXAbcBUwB+xPsqeqDrTabAFuBq6oqqeS/HjrKb5XVa9Z4X5Lkk5Ql0/8lwOzVXWoqo4Au4HrFrT5Z8BtVfUUQFV9c2W7KUlaKV2C/3zgidb6XFPW9grgFUn+d5IvJNnaqjs7yUxT/ubFXiDJDU2bmfn5+WUNYCFnc0rSeBMP9bD4tc8W5uuZwBbgSmAD8L+SvKqqvg1cUFWHk1wE3J3koap6/FlPVrUT2AkwPT19wtkdL9MmSRN1+cQ/B2xsrW8ADi/S5o+q6vtV9VXgIKM3AqrqcPPzEHAvcOlJ9lmSdBK6BP9+YEuSzUnWAtuAhbNz/hD4ewBJ1jM69HMoybokZ7XKrwAOIElaNRMP9VTV0SQ3AvuANcCuqnokyQ5gpqr2NHVXJzkAPAP8m6r6iyQ/DXw0yTFGbzLvb88GkiSdel2O8VNVe4G9C8puaS0X8M7m0W7zOeDVJ99NSdJK8Zu7kjQwvQr+tWeewf/7/jOr3Q1JOq31Kvhffs5ZPPrn3+HI0WOr3RVJOm11Osb/QvGGi1/Ozj8+xCve/VnOftEZ/OjaMzkjIYEzAmckrfV0uoxz128GpMOTdXquFepTl/5IOr1cfN45/Pb253/Ge6+C/99t/RtcdsE6Dv7Zd/jLI0f57pGjHCuoKo4dg6I4VnCsimPHJn9PrOs3ybrc/KXLc1WHJ+rUJ7++LL0gbVz3I6fkdXoV/GvOCFtf9RNsfdVPrHZXJOm01atj/JKkyQx+SRoYg1+SBsbgl6SBMfglaWAMfkkaGINfkgbG4JekgUmXb4ueSknmgT89iadYD3xrhbrzQjG0MQ9tvOCYh+JkxnxhVU11aXjaBf/JSjJTVdOr3Y9TaWhjHtp4wTEPxakas4d6JGlgDH5JGpg+Bv/O1e7AKhjamIc2XnDMQ3FKxty7Y/ySpPH6+IlfkjRGb4I/ydYkB5PMJrlptfuzXEk2JrknyZeTPJLkXzXl5ya5M8ljzc91TXmSfKgZ74NJLms91/VN+8eSXN8q/9tJHmq2+VBOg9t0JVmT5P4kn2nWNyf5YtP330+ytik/q1mfbeo3tZ7j5qb8YJJrWuWn3e9EkpcmuSPJV5p9/boB7ON3NL/TDyf5VJKz+7afk+xK8s0kD7fKnvf9utRrTFRVL/gHsAZ4HLgIWAt8Cbhktfu1zDGcB1zWLL8YeBS4BPiPwE1N+U3Af2iWrwU+y+hOjK8FvtiUnwscan6ua5bXNXV/Aryu2eazwBtPg3G/E/gk8Jlm/XZgW7P8EeBfNMv/EvhIs7wN+P1m+ZJmf58FbG5+D9acrr8TwO8B/7RZXgu8tM/7GDgf+CrwI639+9a+7WfgZ4DLgIdbZc/7fl3qNSb2d7X/I6zQP/rrgH2t9ZuBm1e7Xyc5pj8CrgIOAuc1ZecBB5vljwLbW+0PNvXbgY+2yj/alJ0HfKVV/qx2qzTGDcBdwM8Cn2l+qb8FnLlwvwL7gNc1y2c27bJwXx9vdzr+TgDnNCGYBeV93sfnA080YXZms5+v6eN+Bjbx7OB/3vfrUq8x6dGXQz3Hf7mOm2vKXpCaP28vBb4IvLyqvgHQ/PzxptlSYx5XPrdI+Wr6IPBvgWPN+suAb1fV0Wa93ccfjKupf7ppv9x/h9V0ETAP/G5zeOs/J/kxeryPq+rrwAeArwHfYLTf7qPf+/m4U7Ffl3qNsfoS/Isdx3xBTldK8teATwP/uqr+77imi5TVCZSviiQ/B3yzqu5rFy/StCbUvSDG2ziT0eGAD1fVpcBfMvrzfCkv+DE3x5yvY3R45ieBHwPeuEjTPu3nSVZ9jH0J/jlgY2t9A3B4lfpywpK8iFHo/9eq+oOm+M+TnNfUnwd8sylfaszjyjcsUr5argDelOT/ALsZHe75IPDSJGc2bdp9/MG4mvqXAE+y/H+H1TQHzFXVF5v1Oxi9EfR1HwP8feCrVTVfVd8H/gD4afq9n487Fft1qdcYqy/Bvx/Y0swUWMvopNCeVe7TsjRn6f8L8OWq+s1W1R7g+Nn96xkd+z9e/pZmhsBrgaebP/X2AVcnWdd82rqa0THQbwDfSfLa5rXe0nquU66qbq6qDVW1idH+uruq/hFwD/BLTbOF4z3+7/BLTftqyrc1s0E2A1sYnQg77X4nqurPgCeSvLIpegNwgJ7u48bXgNcm+dGmT8fH3Nv93HIq9utSrzHeap74WeETK9cymgnzOPCu1e7PCfT/7zD68+1B4IHmcS2j45t3AY81P89t2ge4rRnvQ8B067n+CTDbPH6lVT4NPNxs859YcJJxFcd+JT+c1XMRo//Qs8B/A85qys9u1meb+ota27+rGdNBWrNYTsffCeA1wEyzn/+Q0eyNXu9j4N8DX2n69QlGM3N6tZ+BTzE6h/F9Rp/Q33Yq9utSrzHp4Td3JWlg+nKoR5LUkcEvSQNj8EvSwBj8kjQwBr8kDYzBL0kDY/BL0sAY/JI0MP8fhHYMLncx8zMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f260598d400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26059b8390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y)\n",
    "plt.xlim(0,100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2g** There is an asymptote around 0.63."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2h** We will now investigate numerically the probability that a bootstrap sample of size n=100 contains the jth observation. Here j=4. We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.\n",
    "\n",
    ">This value is very close to what we would expect from the equation we derived earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6385"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store = np.random.randint(1, 101, (100, 10000))\n",
    "np.any(store == 4, axis=0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a** Explain how k-fold cross-validation is implemented.\n",
    " \n",
    " > We split the dataset into k non-overlapping groups. We then train K models where each use the k group as a validation set, while training on the other $k-1$ sets. We take the mean of these errors to estimate our models true MSE. \n",
    " \n",
    " **3b** What are the advantages and disadvantages of k-fold crossvalidation relative to:\n",
    "\n",
    "**3bi** The validation set approach?\n",
    "> The estimate of the test error rate can contain a lot of variance and only a subset of the observations are used to train. Most models perform better when trainined with more observations.\n",
    "\n",
    "**3bii** LOOCV?\n",
    "\n",
    "> LOOCV is computationaly expensive and can lead to unbiased estimates of the test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4** Suppose that we use some statistical learning method to make a prediction for the response Y for a particular value of the predictor X. Carefully describe how we might estimate the standard deviation of our prediction.\n",
    "\n",
    "> We can use bootstrap to obtain repated random samples from the original data. We could do this a large amount of times, fiting a model each time and calculating RMSE for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5** In Chapter 4, we used logisitc regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n",
    "\n",
    "**5a** Fit a logistic regression model that uses income and balance to predict default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "default = pd.read_csv('data/default.csv')\n",
    "default['student'] = np.where(default['student'] == 'Yes', 1, 0)\n",
    "default['default'] = np.where(default['default'] == 'Yes', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>default</td>     <th>  No. Observations:  </th>   <td> 10000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  9997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 04 May 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.4594</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:21:52</td>     <th>  Log-Likelihood:    </th>  <td> -789.48</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -1460.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>4.541e-292</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -11.5405</td> <td>    0.435</td> <td>  -26.544</td> <td> 0.000</td> <td>  -12.393   -10.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>    0.0056</td> <td>    0.000</td> <td>   24.835</td> <td> 0.000</td> <td>    0.005     0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td> 2.081e-05</td> <td> 4.99e-06</td> <td>    4.174</td> <td> 0.000</td> <td>  1.1e-05  3.06e-05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                10000\n",
       "Model:                          Logit   Df Residuals:                     9997\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Fri, 04 May 2018   Pseudo R-squ.:                  0.4594\n",
       "Time:                        13:21:52   Log-Likelihood:                -789.48\n",
       "converged:                       True   LL-Null:                       -1460.3\n",
       "                                        LLR p-value:                4.541e-292\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -11.5405      0.435    -26.544      0.000       -12.393   -10.688\n",
       "balance        0.0056      0.000     24.835      0.000         0.005     0.006\n",
       "income      2.081e-05   4.99e-06      4.174      0.000       1.1e-05  3.06e-05\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = smf.logit(formula='default ~ balance + income', data=default).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5b** Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n",
    "\n",
    "**i** Split the sample set into a training set and a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(default[['balance', 'income']], default['default'])\n",
    "X_train_smf = X_train.join(y_train) #for the smf model, might move to SciKit late"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii** Fit a multiple logistic regression model using only the training observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.080439\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>default</td>     <th>  No. Observations:  </th>   <td>  7500</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  7497</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 04 May 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.4546</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:21:53</td>     <th>  Log-Likelihood:    </th>  <td> -603.29</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -1106.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>4.005e-219</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -11.2487</td> <td>    0.485</td> <td>  -23.181</td> <td> 0.000</td> <td>  -12.200   -10.298</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>    0.0055</td> <td>    0.000</td> <td>   21.751</td> <td> 0.000</td> <td>    0.005     0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td> 1.823e-05</td> <td> 5.68e-06</td> <td>    3.212</td> <td> 0.001</td> <td> 7.11e-06  2.94e-05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                 7500\n",
       "Model:                          Logit   Df Residuals:                     7497\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Fri, 04 May 2018   Pseudo R-squ.:                  0.4546\n",
       "Time:                        13:21:53   Log-Likelihood:                -603.29\n",
       "converged:                       True   LL-Null:                       -1106.2\n",
       "                                        LLR p-value:                4.005e-219\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -11.2487      0.485    -23.181      0.000       -12.200   -10.298\n",
       "balance        0.0055      0.000     21.751      0.000         0.005     0.006\n",
       "income      1.823e-05   5.68e-06      3.212      0.001      7.11e-06  2.94e-05\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.13 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = smf.logit(formula='default ~ balance + income', data=X_train_smf).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii** Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the  default category if the posterior probability is greater than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.where(results.predict(X_test) > .5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iv** Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0264"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(y_test - pred).sum() / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5c** Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.077259\n",
      "         Iterations 10\n",
      "Validation Set Error:  0.0264\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.077419\n",
      "         Iterations 10\n",
      "Validation Set Error:  0.0288\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.076175\n",
      "         Iterations 10\n",
      "Validation Set Error:  0.0288\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(default[['balance', 'income']], default['default'])\n",
    "    X_train_smf = X_train.join(y_train)\n",
    "    results = smf.logit(formula='default ~ balance + income', data=X_train_smf).fit()\n",
    "    pred = np.where(results.predict(X_test) > .5, 1, 0)\n",
    "    print('Validation Set Error: ', np.abs(y_test - pred).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation set error can be vary, depending on which observations we include, but in this case not by a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5d** Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. \n",
    "\n",
    "Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.075799\n",
      "         Iterations 10\n",
      "Validation Set Error:  0.0296\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.079860\n",
      "         Iterations 10\n",
      "Validation Set Error:  0.026\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078326\n",
      "         Iterations 10\n",
      "Validation Set Error:  0.0272\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(default[['balance', 'income', 'student']], \n",
    "                                                    default['default'])\n",
    "    X_train_smf = X_train.join(y_train)\n",
    "    results = smf.logit(formula='default ~ balance + income + student', data=X_train_smf).fit()\n",
    "    pred = np.where(results.predict(X_test) > .5, 1, 0)\n",
    "    print('Validation Set Error: ', np.abs(y_test - pred).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the validation set error is pretty similar, it does have much more variance when student is included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6** We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now computes estimates for the standard errors of the income and balance logistic regression coefficients in two different ways:\n",
    "\n",
    "+ (1) using the bootstrap\n",
    "+ (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\n",
    "\n",
    "**6a** Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with âincomeâ and âbalanceâ in a multiple logistic regression model that uses both predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078948\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>default</td>     <th>  No. Observations:  </th>   <td> 10000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  9997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 04 May 2018</td> <th>  Pseudo R-squ.:     </th>   <td>0.4594</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:21:54</td>     <th>  Log-Likelihood:    </th>  <td> -789.48</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th>  <td> -1460.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>4.541e-292</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -11.5405</td> <td>    0.435</td> <td>  -26.544</td> <td> 0.000</td> <td>  -12.393   -10.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>    0.0056</td> <td>    0.000</td> <td>   24.835</td> <td> 0.000</td> <td>    0.005     0.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>income</th>    <td> 2.081e-05</td> <td> 4.99e-06</td> <td>    4.174</td> <td> 0.000</td> <td>  1.1e-05  3.06e-05</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                default   No. Observations:                10000\n",
       "Model:                          Logit   Df Residuals:                     9997\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Fri, 04 May 2018   Pseudo R-squ.:                  0.4594\n",
       "Time:                        13:21:54   Log-Likelihood:                -789.48\n",
       "converged:                       True   LL-Null:                       -1460.3\n",
       "                                        LLR p-value:                4.541e-292\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -11.5405      0.435    -26.544      0.000       -12.393   -10.688\n",
       "balance        0.0056      0.000     24.835      0.000         0.005     0.006\n",
       "income      2.081e-05   4.99e-06      4.174      0.000       1.1e-05  3.06e-05\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.14 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = smf.logit(formula='default ~ balance + income', data=default).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model summary from statsmodels indicates that the standard error for $\\beta_0$, $\\beta_1$, and $\\beta_2$ are $0.435$, $0.000$, and $4.99e^{-06}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6b** Write a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for âincomeâ and âbalanceâ in the multiple logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boot(data, obs):\n",
    "    X_train = data.iloc[obs]\n",
    "    results = smf.logit(formula='default ~ balance + income', data=X_train).fit(disp=0)\n",
    "    #thanks to stackoverflow for the disp=0 trick, when I ran bootstrap below\n",
    "    #the output was dreadful\n",
    "    return results.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.random.choice(default.index, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept   -14.162901\n",
       "balance       0.006099\n",
       "income        0.000082\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot(default, obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6c** Use the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance. \n",
    "\n",
    "*Not exactly sure what the boot function does, but I can work it out.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data, num_samples, runs):\n",
    "    beta_df = pd.DataFrame(columns=['Intercept', 'balance', 'income'])\n",
    "    for i in range(runs):\n",
    "        obs = np.random.choice(default.index, num_samples)\n",
    "        coefs = boot(data, obs)\n",
    "        beta_df = beta_df.append(coefs, ignore_index=True)\n",
    "    \n",
    "    return beta_df.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept    1.588325\n",
       "balance      0.000807\n",
       "income       0.000016\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap(default, 1000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6d** Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function.\n",
    "\n",
    "> The estimated standard errors are similar with the ones coming from the bootstrap method a little higher.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard errors are a wee bit higher in bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7** In sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4).\n",
    "\n",
    "**7a** Fit a logistic regression model that predicts âDirectionâ using âLag1â and âLag2â."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly = pd.read_csv('data/weekly.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly['Direction'] = np.where(weekly['Direction'] == 'Up', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>Direction</td>    <th>  No. Observations:  </th>  <td>  1089</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1086</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 04 May 2018</td> <th>  Pseudo R-squ.:     </th> <td>0.005335</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:22:00</td>     <th>  Log-Likelihood:    </th> <td> -744.11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -748.10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td>0.01848</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.2212</td> <td>    0.061</td> <td>    3.599</td> <td> 0.000</td> <td>    0.101     0.342</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag1</th>      <td>   -0.0387</td> <td>    0.026</td> <td>   -1.477</td> <td> 0.140</td> <td>   -0.090     0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>    0.0602</td> <td>    0.027</td> <td>    2.270</td> <td> 0.023</td> <td>    0.008     0.112</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              Direction   No. Observations:                 1089\n",
       "Model:                          Logit   Df Residuals:                     1086\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Fri, 04 May 2018   Pseudo R-squ.:                0.005335\n",
       "Time:                        13:22:00   Log-Likelihood:                -744.11\n",
       "converged:                       True   LL-Null:                       -748.10\n",
       "                                        LLR p-value:                   0.01848\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.2212      0.061      3.599      0.000         0.101     0.342\n",
       "Lag1          -0.0387      0.026     -1.477      0.140        -0.090     0.013\n",
       "Lag2           0.0602      0.027      2.270      0.023         0.008     0.112\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = smf.logit(formula='Direction ~ Lag1 + Lag2', data=weekly).fit(disp=0)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7b** Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = weekly.iloc[1:]\n",
    "test = weekly.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>Direction</td>    <th>  No. Observations:  </th>  <td>  1088</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  1085</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Fri, 04 May 2018</td> <th>  Pseudo R-squ.:     </th> <td>0.005387</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>13:22:01</td>     <th>  Log-Likelihood:    </th> <td> -743.26</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -747.29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>  <td>0.01785</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.2232</td> <td>    0.061</td> <td>    3.630</td> <td> 0.000</td> <td>    0.103     0.344</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag1</th>      <td>   -0.0384</td> <td>    0.026</td> <td>   -1.466</td> <td> 0.143</td> <td>   -0.090     0.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Lag2</th>      <td>    0.0608</td> <td>    0.027</td> <td>    2.291</td> <td> 0.022</td> <td>    0.009     0.113</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:              Direction   No. Observations:                 1088\n",
       "Model:                          Logit   Df Residuals:                     1085\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Fri, 04 May 2018   Pseudo R-squ.:                0.005387\n",
       "Time:                        13:22:01   Log-Likelihood:                -743.26\n",
       "converged:                       True   LL-Null:                       -747.29\n",
       "                                        LLR p-value:                   0.01785\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.2232      0.061      3.630      0.000         0.103     0.344\n",
       "Lag1          -0.0384      0.026     -1.466      0.143        -0.090     0.013\n",
       "Lag2           0.0608      0.027      2.291      0.022         0.009     0.113\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = smf.logit(formula='Direction ~ Lag1 + Lag2', data=train).fit(disp=0)\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7c** Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if $P(direction = Up|Lag1,Lag2)>0.5$. Was this observation correctly classified?\n",
    "\n",
    "> It was not classified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.where(results.predict(test) > .5, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7d** Write a loop from $i=1$ to $i=n$, where $n$ is the number of observations in the data set, that performs each of the following steps:\n",
    "\n",
    "+ Fit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.\n",
    "\n",
    "+ Compute the posterior probability of the market moving up for the ith observation.\n",
    "\n",
    "+ Use the posterior probability for the ith observation in order to predict whether or not the market moves up.\n",
    "\n",
    "+ Determine whether or not an error was made in predicting the direction for the ith observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []\n",
    "for i in range(weekly.shape[0]):\n",
    "    loo = [j for j in weekly.index if j != i]\n",
    "    train = weekly.iloc[loo]\n",
    "    test = weekly.iloc[i]\n",
    "    results = smf.logit(formula='Direction ~ Lag1 + Lag2', data=train).fit(disp=0)\n",
    "    pred = np.where(results.predict(test) > .5, 1.0, 0.0)\n",
    "    if pred == test.Direction:\n",
    "        errors.append(0)\n",
    "    else:\n",
    "        errors.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44995408631772266"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8** We will now perform cross-validation on a simulated data set.\n",
    "\n",
    "**8a** Generate a simulated data set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x = np.random.normal(size=100)\n",
    "y = x - 2*x**2 + np.random.normal(size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this data set, what is $n$ and what is $p$? Write out the model used to generate the data in equation form.\n",
    "\n",
    ">$n=100$, $p=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8b** Create a scatterplot of $X$ against $Y$. Comment on what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26054847f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the quadratic relationship present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares:\n",
    "    \n",
    "Note you may find it helpful to use the data.frame() function to create a single dataset containing both X and Y ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(np.array([x, x ** 2, x ** 3, x ** 4, y]).T, columns=['x', 'x2', 'x3', 'x4', 'y'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = {}\n",
    "for i in range(4):\n",
    "    model_errors = []\n",
    "    form = 'y ~ '\n",
    "    for j in range(0,i+1):\n",
    "        if j == 0:\n",
    "            form += train.columns[j]\n",
    "        else:\n",
    "            form += ' + ' + train.columns[j]\n",
    "    for ite in range(train.shape[0]):\n",
    "        loo = [j for j in df.index if j != i]\n",
    "        train = df.iloc[loo]\n",
    "        test = df.iloc[ite]\n",
    "\n",
    "        results = smf.ols(formula=form, data=train).fit()\n",
    "        model_errors.append((test.y - results.predict(test)))\n",
    "\n",
    "    \n",
    "    errors[df.columns[i]] = np.mean(model_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the MSE for each model, it looks like the cubic model performed best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 0.033298208034557596,\n",
       " 'x2': 0.016775059294228015,\n",
       " 'x3': -0.008152255197720493,\n",
       " 'x4': -0.01902393816782145}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8d** Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c) ? Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 0.033298208034557596,\n",
       " 'x2': 0.016775059294228015,\n",
       " 'x3': -0.008152255197720493,\n",
       " 'x4': -0.01902393816782145}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(23) #MJ\n",
    "x = np.random.normal(size=100)\n",
    "y = x - 2*x**2 + np.random.normal(size=100)\n",
    "\n",
    "df = pd.DataFrame(np.array([x, x ** 2, x ** 3, x ** 4, y]).T, columns=['x', 'x2', 'x3', 'x4', 'y'])\n",
    "\n",
    "errors = {}\n",
    "for i in range(4):\n",
    "    model_errors = []\n",
    "    form = 'y ~ '\n",
    "    for j in range(0,i+1):\n",
    "        if j == 0:\n",
    "            form += train.columns[j]\n",
    "        else:\n",
    "            form += ' + ' + train.columns[j]\n",
    "    for ite in range(train.shape[0]):\n",
    "        loo = [j for j in df.index if j != i]\n",
    "        train = df.iloc[loo]\n",
    "        test = df.iloc[ite]\n",
    "\n",
    "        results = smf.ols(formula=form, data=train).fit()\n",
    "        model_errors.append((test.y - results.predict(test)))\n",
    "\n",
    "    \n",
    "    errors[df.columns[i]] = np.mean(model_errors)\n",
    "\n",
    "errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am getting the same results as the last time, because LOOCV evaluates $n$ folds of a single observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8e** Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer.\n",
    "\n",
    "> I expected the quadratic to recieive the lowest prediction error to come from the quadratic fit, but it was actually the cubic that produced the error rate closest to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y ~ x\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.004\n",
      "Model:                            OLS   Adj. R-squared:                 -0.006\n",
      "Method:                 Least Squares   F-statistic:                    0.3927\n",
      "Date:                Fri, 04 May 2018   Prob (F-statistic):              0.532\n",
      "Time:                        13:55:26   Log-Likelihood:                -245.79\n",
      "No. Observations:                  99   AIC:                             495.6\n",
      "Df Residuals:                      97   BIC:                             500.8\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -1.7965      0.296     -6.072      0.000        -2.384    -1.209\n",
      "x              0.1948      0.311      0.627      0.532        -0.422     0.812\n",
      "==============================================================================\n",
      "Omnibus:                       73.366   Durbin-Watson:                   1.946\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              421.256\n",
      "Skew:                          -2.456   Prob(JB):                     3.35e-92\n",
      "Kurtosis:                      11.831   Cond. No.                         1.13\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "y ~ x + x2\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.880\n",
      "Model:                            OLS   Adj. R-squared:                  0.877\n",
      "Method:                 Least Squares   F-statistic:                     351.6\n",
      "Date:                Fri, 04 May 2018   Prob (F-statistic):           6.63e-45\n",
      "Time:                        13:55:26   Log-Likelihood:                -141.09\n",
      "No. Observations:                  99   AIC:                             288.2\n",
      "Df Residuals:                      96   BIC:                             296.0\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.1303      0.121     -1.077      0.284        -0.370     0.110\n",
      "x              0.9799      0.112      8.712      0.000         0.757     1.203\n",
      "x2            -1.9268      0.073    -26.457      0.000        -2.071    -1.782\n",
      "==============================================================================\n",
      "Omnibus:                        1.247   Durbin-Watson:                   2.259\n",
      "Prob(Omnibus):                  0.536   Jarque-Bera (JB):                1.316\n",
      "Skew:                          -0.214   Prob(JB):                        0.518\n",
      "Kurtosis:                       2.632   Cond. No.                         2.34\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "y ~ x + x2 + x3\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.881\n",
      "Model:                            OLS   Adj. R-squared:                  0.877\n",
      "Method:                 Least Squares   F-statistic:                     234.4\n",
      "Date:                Fri, 04 May 2018   Prob (F-statistic):           9.08e-44\n",
      "Time:                        13:55:26   Log-Likelihood:                -140.63\n",
      "No. Observations:                  99   AIC:                             289.3\n",
      "Df Residuals:                      95   BIC:                             299.6\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.1569      0.124     -1.262      0.210        -0.404     0.090\n",
      "x              1.0922      0.164      6.655      0.000         0.766     1.418\n",
      "x2            -1.8904      0.083    -22.914      0.000        -2.054    -1.727\n",
      "x3            -0.0391      0.042     -0.940      0.350        -0.122     0.043\n",
      "==============================================================================\n",
      "Omnibus:                        1.296   Durbin-Watson:                   2.258\n",
      "Prob(Omnibus):                  0.523   Jarque-Bera (JB):                1.371\n",
      "Skew:                          -0.231   Prob(JB):                        0.504\n",
      "Kurtosis:                       2.654   Cond. No.                         7.21\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "y ~ x + x2 + x3 + x4\n",
      "\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.883\n",
      "Model:                            OLS   Adj. R-squared:                  0.878\n",
      "Method:                 Least Squares   F-statistic:                     177.3\n",
      "Date:                Fri, 04 May 2018   Prob (F-statistic):           6.91e-43\n",
      "Time:                        13:55:26   Log-Likelihood:                -139.80\n",
      "No. Observations:                  99   AIC:                             289.6\n",
      "Df Residuals:                      94   BIC:                             302.6\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -0.0798      0.138     -0.577      0.565        -0.354     0.195\n",
      "x              1.1864      0.180      6.595      0.000         0.829     1.544\n",
      "x2            -2.0697      0.164    -12.595      0.000        -2.396    -1.743\n",
      "x3            -0.0801      0.053     -1.520      0.132        -0.185     0.025\n",
      "x4             0.0319      0.025      1.260      0.211        -0.018     0.082\n",
      "==============================================================================\n",
      "Omnibus:                        0.935   Durbin-Watson:                   2.266\n",
      "Prob(Omnibus):                  0.627   Jarque-Bera (JB):                1.026\n",
      "Skew:                          -0.163   Prob(JB):                        0.599\n",
      "Kurtosis:                       2.622   Cond. No.                         25.2\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    model_errors = []\n",
    "    form = 'y ~ '\n",
    "    for j in range(0,i+1):\n",
    "        if j == 0:\n",
    "            form += train.columns[j]\n",
    "        else:\n",
    "            form += ' + ' + train.columns[j]\n",
    "    results = smf.ols(formula=form, data=train).fit()\n",
    "    print(form)\n",
    "    print('')\n",
    "    print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8f** Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?\n",
    "\n",
    "The linear and quadratic term gain significance in each model, and all four polynomial models perform very similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9** We will now consider the Boston housing data set.\n",
    "\n",
    "**9a** Based on this data set, provide an estimate for the population mean of medv. Call this estimate $\\hat{\\mu}$.\n",
    "\n",
    "> 22.532"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.read_csv('data/boston.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.532806324110698"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['medv'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9b** Provide an estimate of the standard error of $\\hat{\\mu}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4088611474975351"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['medv'].std() / np.sqrt(boston.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9c** Now estimate the standard error of $\\hat{\\mu}$ using the bootstrap. How does this compare to your answer from (b)?\n",
    "\n",
    "> This is extremely close to the value we got in part b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40886002940119864"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_means = [boston['medv'].sample(n=len(boston), replace=True).mean() for i in range(10000)]\n",
    "np.std(sample_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9d** Based on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using `st.t.interval()`.\n",
    "\n",
    "> Again, the two values are very close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.72089832064425, 23.356338438249043)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boot_se = np.std(sample_means)\n",
    "np.mean(sample_means) - 2 * boot_se, np.mean(sample_means) + 2 * boot_se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.729528014578616, 23.33608463364278)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.t.interval(0.95, boston.shape[0]-1, loc=boston['medv'].mean(), scale=st.sem(boston['medv']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9e** Based on this data set, provide an estimate, $\\hat{\\mu_{\\text{med}}}$, for the median value of medv in the population.\n",
    "\n",
    "> 21.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.2"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['medv'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now would like to estimate the standard error of $\\hat{\\mu_{\\text{med}}}$. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.\n",
    "\n",
    "> We get an estimated median almost equal to the value obtained in (e), with a standard error of 0.38777."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.182660000000002 0.3777874063544201\n"
     ]
    }
   ],
   "source": [
    "sample_medians = [boston['medv'].sample(n = boston.shape[0], replace=True).median() for i in range(10000)]\n",
    "print(np.mean(sample_medians), np.std(sample_medians))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9g** Based on this data set, provide an estimate for the tenth percentile of medv in Boston suburbs. Call this quantity $\\hat{\\mu_{0.1}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.75"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston['medv'].quantile(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**9h** Use the bootstrap to estimate the standard error of $\\hat{\\mu_{0.1}}$. Comment on your findings.\n",
    "\n",
    "> Our estiamted 10th quantile value is 12.75105, which is essntially equal to the sample value, with a standard error of 0.5081."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.75105 0.50814062768096\n"
     ]
    }
   ],
   "source": [
    "sample_quant_10 = [boston['medv'].sample(n = boston.shape[0], replace=True).quantile(.1) for i in range(10000)]\n",
    "print(np.mean(sample_quant_10), np.std(sample_quant_10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
