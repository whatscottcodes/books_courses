{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import ts_code.nsfg as nsfg\n",
    "import ts_code.thinkstats2 as thinkstats2\n",
    "import ts_code.thinkplot as thinkplot\n",
    "import ts_code.first as first\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9 - Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental question we want to address is whether the effects we see in a sample are likely to appear in the larger population.\n",
    "\n",
    "There are several ways we could formulate this question, including Fisher null hypothesis testing, Neyman-Pearson decision theory, and Bayesian inference.\n",
    "\n",
    "In this chapter we walk through a subset of all three called **classical hypothesis testing.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we answer the question above:\n",
    "\n",
    "+ The first step is to quantify the size of the apparent effect by choosing a **test statistic**. \n",
    "\n",
    "+ The second step is to define a **null hypothesis**, which is a model of the system based on the assumption that the apparent effect is not real.\n",
    "\n",
    "+ The third step is to compute a **p-value**, which is the probability of seeing the apparent effect if the null hypothesis is true.\n",
    "\n",
    "+ The last step is to interpret the result. If the p-value is low, the effect is said to be **statistically significant**, which means that it is unlikely to have occurred by chance. In that case we infer that the effect is more likely to appear in the larger population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common effects to test is a difference in means between two groups. \n",
    "\n",
    "In the NSFG data, we saw that the mean pregnancy length for first babies is slightly longer, and the mean birth weight is slightly smaller.\n",
    "\n",
    "\n",
    "For these examples, the null hypothesis is that the distributions for the two\n",
    "groups are the same. One way to model the null hypothesis is by **permutation**; that is, we can take values for first babies and others and shuffle them,\n",
    "treating the two groups as one big group.\n",
    "\n",
    "So, we take the two groups, combine them, shuffle, and then partition them back out into two groups and check the difference in mean at the end of each trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the best test statistic depends on what question you are trying to\n",
    "address.\n",
    "\n",
    "If we had some reason to think that first babies are likely to be late, then we would not take the absolute value of the difference; instead we would use a onesided difference.\n",
    "\n",
    "This kind of test is called **one-sided** because it only counts one side of the distribution of differences.\n",
    "\n",
    "The previous test, using both sides, is **two-sided**.\n",
    "\n",
    "In general the p-value for a one-sided test is about half the p-value for a two-sided test, depending on the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These same methods can be used to test correlations, we can use Pearson's correlation or Spearman's as the test statistic. If we have reason to expect positive correlation, we could use a one-sided test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also test proportions by comparing observed values vs expected values. \n",
    "\n",
    "If you have a die that comes up 3, 19 times in 60 trials and 4 only 5 times, you might think it crooked. To test this out test statistic is the sum of the absolute difference between these observed values and our expected value of every face coming up 10 times.\n",
    "\n",
    "Next, we run a model and see how often a deviation this big or bigger comes up, this gives us a p-value to use to decided if this is a statistically signifigant deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More often, instead of using the total deviation as a test statistic for testing proportion, we will use the **chi-squared** statistic;\n",
    "\n",
    "$$\\chi^{2} = \\sum_{i} \\frac{(O_{i}-E_{i})^{2}}{E_{i}}$$\n",
    "\n",
    "where $O_{i}$ is the observed frequencies and $E_{i}$ are the expected frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squaring the deviations (rather than taking absolute values) gives more weight to large deviations. Dividing through by expected standardizes the deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the p-value depends on the choice of test statistic and the model of the null hypothesis, and sometimes these choices determine whether an effect is statistically significant or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the effect is actually due to change, what is the probability that we will wrongly consider it significant? This is the **false positive rate**.\n",
    "\n",
    "If the effect if real, what is the chance that the hypothesis test will fail? This is the **false negative rate**.\n",
    "\n",
    "The false positive rate is relatively easy to compute: if the threshold is 5%,\n",
    "the false positive rate is 5%.\n",
    "\n",
    "If there is no effect, we can simulate the null hypothesis and the distribution of test statistics from the null hypothesis. The p-value is the probability that a random value exceeds this test statistic, which is 1-CDF(t). If the p-value is less than 5%, then CDF(t) is greater than 95%, and this happens 5% of the itme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The false negative rate depends on the *effect size* and normally we don't know that. So, we can compute a rate conditioned on a hypothetical effect size.\n",
    "\n",
    "We assume the observed difference is accurate, then use the observed sample as a model of the population, running hypothesis tests with simulated data.\n",
    "\n",
    "We would sample with replacement from out two groups, run the hypothesis test, check the result and count the number of false negatives, that is outcomes with a p_value above our threshold(normally 5%).\n",
    "\n",
    "This will give us the percent of time we expect an experiment with this sample size to yield a negative test. Generally this is presented as 1-negative rate, which is the percent we expect a postive test.\n",
    "\n",
    "This \"correct positive rate\" is called the **power** of the test or \"sensitivity\". It reflects the ability of the test to detect an effect of a given size.\n",
    "\n",
    "As a rule of thumb, a power of 80% is considered acceptable, anything below this is \"underpowered\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general a negative hypothesis test does not imply that there is no difference\n",
    "between the groups; instead it suggests that if there is a difference, it is too\n",
    "small to detect with this sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing prodcedure outlined above is problematic for a few reasons. We performed multiple test, running one hypothesis test has a false positive rate of %5, running 20 means you're almost guranteed to have one.\n",
    "\n",
    "The same datset was also used for exploration and testing. If you explore a large dataset, find a surprising effect, and then test whether it is significant, you have a good chance of generating a false positive.\n",
    "\n",
    "To compensate for multiple tests, you can adjust the p-value threshold using the Holm-Donferroni method;\n",
    "\n",
    "+ Order the p-values of the null hypothesis from lowest to highest, for a given significance level $\\alpha$, let $k$ by the minimal index such that; $P_{k}>\\frac{\\alpha}{m + 1 - k}$.\n",
    "\n",
    "+ Reject the null hypothesis $H_{1}\\dots H_{k-1}$ and do not reject $H_{k}\\dots H_{m}$. If $k=1$ do not reject any nulls, if no $k$ exists, reject all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can address both problems by partitioning the data, using one set for exploration and the other for testing.\n",
    "\n",
    "It is also common to address these problems implicitly by replicating published\n",
    "results. Typically the first paper to report a new result is considered exploratory. Subsequent papers that replicate the result with new data are considered confirmatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Exercise 9.1** As sample size increases, the power of a hypothesis test increases, which means it is more likely to be positive if the effect is real. Conversely, as sample size decreases, the test is less likely to be positive even if the effect is real.\n",
    "\n",
    "To investigate this behavior, run the tests in this chapter with different subsets of the NSFG data. You can use thinkstats2.SampleRows to select a random subset of the rows in a DataFrame.\n",
    "\n",
    "What happens to the p-values of these tests as sample size decreases? \n",
    ">p-values grow as the sample size decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nsfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c09756c60eda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mts_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhypothesis\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhyp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/scott/ds/thinkStats/ts_code/hypothesis.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnsfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnsfg2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nsfg'"
     ]
    }
   ],
   "source": [
    "import ts_code.hypothesis as hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "live, firsts, others = first.MakeFrames()\n",
    "live = live.dropna(subset=['agepreg', 'totalwgt_lb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = range(150, 4400, 500)\n",
    "prglngth_ps=[]\n",
    "agewgth_ps= []\n",
    "weightdiff_ps = []\n",
    "chi_ps = []\n",
    "\n",
    "for n in sample_sizes:\n",
    "    #sample data\n",
    "    live_sample = thinkstats2.SampleRows(live, n)\n",
    "    firsts_sample = thinkstats2.SampleRows(firsts, n)\n",
    "    others_sample = thinkstats2.SampleRows(others, n)\n",
    "    \n",
    "    #difference in pregnancy lenghts\n",
    "    data = firsts_sample.prglngth.values, others_sample.prglngth.values\n",
    "    ht = hyp.DiffMeansPermute(data)\n",
    "    prglngth_pvalue = ht.PValue(1000)\n",
    "    \n",
    "    #mean weight difference, first vs others\n",
    "    data = (firsts_sample.totalwgt_lb.dropna().values,\n",
    "            others_sample.totalwgt_lb.dropna().values)\n",
    "    ht = hyp.DiffMeansPermute(data)\n",
    "    weightdiff_pvalue = ht.PValue(1000)\n",
    "    \n",
    "    #correlation between mom age and birth weight\n",
    "    data = live_sample.agepreg.values, live_sample.totalwgt_lb.values\n",
    "    ht = hyp.CorrelationPermute(data)\n",
    "    age_bwght_pvalue = ht.PValue(1000)\n",
    "    \n",
    "    #pregnancy lenghts(chi squared)\n",
    "    data = firsts_sample.prglngth.values, others_sample.prglngth.values\n",
    "    ht = hyp.PregLengthTest(data)\n",
    "    chi_pvalue = ht.PValue(1000)\n",
    "    \n",
    "    prglngth_ps.append(prglngth_pvalue)\n",
    "    agewgth_ps.append(age_bwght_pvalue)\n",
    "    weightdiff_ps.append(weightdiff_pvalue)\n",
    "    chi_ps.append(chi_pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample_sizes, prglngth_ps, label = 'Pregnancy Length' )\n",
    "plt.plot(sample_sizes, agewgth_ps, label = 'Age/Weight' )\n",
    "plt.plot(sample_sizes, weightdiff_ps, label = 'Weight Mean Difference' )\n",
    "plt.plot(sample_sizes, chi_ps, label = 'Pregnancy Length(Chi)' )\n",
    "plt.ylabel('p-value')\n",
    "plt.xlabel('sample size')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 9.2** In Section 9.3, we simulated the null hypothesis by permutation; that is, we treated the observed values as if they represented the entire population, and randomly assigned the members of the population to the\n",
    "two groups.\n",
    "\n",
    "An alternative is to use the sample to estimate the distribution for the population, then draw a random sample from that distribution. This process is called resampling. There are several ways to implement resampling, but one of the simplest is to draw a sample with replacement from the observed values, as in Section 9.10.\n",
    "\n",
    "Write a class named *DiffMeansResample* that inherits from *DiffMeansPermute* and overrides RunModel to implement resampling, rather than permutation.\n",
    "\n",
    "Use this model to test the differences in pregnancy length and birth weight.\n",
    "How much does the model affect the results?\n",
    "\n",
    ">In this situation it has a very small affect, though after running the test a few times, the result were identical more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffMeansPermute(thinkstats2.HypothesisTest):\n",
    "    \"\"\"Tests a difference in means by permutation.\"\"\"\n",
    "\n",
    "    def TestStatistic(self, data):\n",
    "        \"\"\"Computes the test statistic.\n",
    "\n",
    "        data: data in whatever form is relevant        \n",
    "        \"\"\"\n",
    "        group1, group2 = data\n",
    "        test_stat = abs(group1.mean() - group2.mean())\n",
    "        return test_stat\n",
    "\n",
    "    def MakeModel(self):\n",
    "        \"\"\"Build a model of the null hypothesis.\n",
    "        \"\"\"\n",
    "        group1, group2 = self.data\n",
    "        self.n, self.m = len(group1), len(group2)\n",
    "        self.pool = np.hstack((group1, group2))\n",
    "\n",
    "    def RunModel(self):\n",
    "        \"\"\"Run the model of the null hypothesis.\n",
    "\n",
    "        returns: simulated data\n",
    "        \"\"\"\n",
    "        np.random.shuffle(self.pool)\n",
    "        data = self.pool[:self.n], self.pool[self.n:]\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffMeansResample(DiffMeansPermute):\n",
    "    \n",
    "    def RunModel(self):\n",
    "        sample = thinkstats2.Resample(self.pool)\n",
    "        data = sample[:self.n], sample[self.n:]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#difference in pregnancy lenghts\n",
    "data = firsts.prglngth.values, others.prglngth.values\n",
    "ht = hyp.DiffMeansPermute(data)\n",
    "prglngth_perm = ht.PValue(1000)\n",
    "    \n",
    "#mean weight difference, first vs others\n",
    "data = (firsts.totalwgt_lb.dropna().values,\n",
    "            others.totalwgt_lb.dropna().values)\n",
    "ht = hyp.DiffMeansPermute(data)\n",
    "weightdiff_perm = ht.PValue(1000)\n",
    "    \n",
    "#difference in pregnancy lenghts(ReSample)\n",
    "data = firsts.prglngth.values, others.prglngth.values\n",
    "ht = DiffMeansResample(data)\n",
    "prglngth_reshuf= ht.PValue(1000)\n",
    "    \n",
    "#mean weight difference, first vs others(Resample)\n",
    "data = (firsts.totalwgt_lb.dropna().values,\n",
    "            others.totalwgt_lb.dropna().values)\n",
    "ht = DiffMeansResample(data)\n",
    "weightdiff_reshuf = ht.PValue(1000)\n",
    "\n",
    "print('Permuation: (%.05f, %.05f), Resample:(%.05f, %.05f)' % \n",
    "          (prglngth_perm, weightdiff_perm, prglngth_reshuf, weightdiff_reshuf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glossary\n",
    "\n",
    "**hypothesis testing:** The process of determining whether an apparent effect is statistically significant.\n",
    "\n",
    "**test statistic:** A statistic used to quantify an effect size.\n",
    "\n",
    "**null hypothesis:** A model of a system based on the assumption that an apparent effect is due to chance.\n",
    "\n",
    "**p-value:** The probability that an effect could occur by chance.\n",
    "\n",
    "**statistically significant:** An effect is statistically significant if it is unlikely to occur by chance.\n",
    "\n",
    "**permutation test:** A way to compute p-values by generating permutations of an observed dataset.\n",
    "\n",
    "**resampling test:** A way to compute p-values by generating samples, with replacement, from an observed dataset.\n",
    "\n",
    "**two-sided test:** A test that asks, \"What is the chance of an effect as big as the observed effect, positive or negative?\"\n",
    "\n",
    "**one-sided test:** A test that asks, \"What is the chance of an effect as big as the observed effect, and with the same sign?\"\n",
    "\n",
    "**chi-squared test:** A test that uses the chi-squared statistic as the test statistic.\n",
    "\n",
    "**false positive:** The conclusion that an effect is real when it is not.\n",
    "\n",
    "**false negative:** The conclusion that an effect is due to chance when it is not.\n",
    "\n",
    "**power:** The probability of a positive test if the null hypothesis is false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
