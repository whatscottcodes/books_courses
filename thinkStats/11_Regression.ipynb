{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import ts_code.nsfg as nsfg\n",
    "import ts_code.thinkstats2 as thinkstats2\n",
    "import ts_code.thinkplot as thinkplot\n",
    "import ts_code.first as first\n",
    "import matplotlib.pyplot as plt\n",
    "import ts_code.brfss as brfss\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 11 - Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression** is the problem of fitting any kind of model to any kind of data. The goal is to describe the relationship between one set of variables, called **dependent variables** and another set of variables, called **independent variables**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there is only one dependent and one explanatory variable, that's **simple regression**.\n",
    "\n",
    "Regression with more than one explanatory variable is **multiple regression**, if there is more than one dependent variable, that is **multivariate regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the relationship between the dependent and explanatory variable is linear, that's **linear regression**.\n",
    "\n",
    "If $y$ is the dependent, and $x_{1}$, $x_{2}$ are explanatory variables, the following is a linear regression model;\n",
    "\n",
    "$$y = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\epsilon$$\n",
    "\n",
    "where $\\beta_{0}$ is the intercept, $\\beta_{1}$ and $\\beta_{2}$ are parameters and $\\epsilon$ is the residual due to random variation or other unknown factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sequence of $y$, $x_{1}$, and $x_{2}$ values we can find parameters $\\beta_{0}$, $\\beta_{1}$, and $\\beta_{2}$ that minimize the sum of $\\epsilon^{2}$. This called **ordinary least squares**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**StatsModels** is  a Python package that provides several forms of regression and other analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model from least squares\n",
    "live, firsts, others = first.MakeFrames()\n",
    "formula = 'totalwgt_lb ~ agepreg'\n",
    "model = smf.ols(formula, data=live)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`statsmodels` provides two interfaces (APIs); the \"formula\" API uses strings to identify the dependent and explanatory variables. It uses a syntax called *patsy*; in this example, the **~** operator separates the dependent variable on the left from the explanatory variables on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`smf.ols` takes the formula string and the DataFrame, live, and returns an OLS object that represents the model. The name ols stands for **ordinary least squares**.\n",
    "\n",
    "The `fit` method fits the model to the data and returns a *RegressionResults* object that contains the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are available as attributes. `params` is a Series that maps from variable names to their parameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter = results.params['Intercept']\n",
    "slope = results.params['agepreg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept 6.83039697331\n",
      "agepreg 0.0174538514718\n"
     ]
    }
   ],
   "source": [
    "for key, val in results.params.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7229471073134105e-11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope_pvalues = results.pvalues['agepreg']\n",
    "slope_pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept 0.0\n",
      "agepreg 5.72294710731e-11\n"
     ]
    }
   ],
   "source": [
    "for key, val in results.pvalues.items():\n",
    "    print(key, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0047381154747102583"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = results.rsquared\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`results` also provides *f_pvalue*, which is the p-value associated with the model as a whole, similar to testing whether R2 is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.7229471072556979e-11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_p = results.f_pvalue\n",
    "f_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Results` also provides *resid*, a sequence of residuals, and *fittedvalues*, a\n",
    "sequence of fitted values corresponding to agepreg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.403333\n",
       "1        0.359539\n",
       "2        2.044489\n",
       "3       -0.141599\n",
       "4       -0.962826\n",
       "5        1.260849\n",
       "6        2.228908\n",
       "7        1.018195\n",
       "8        0.241999\n",
       "9       -0.769680\n",
       "10       0.532666\n",
       "11      -0.231836\n",
       "12      -3.259413\n",
       "15       0.362635\n",
       "16       0.140228\n",
       "17      -0.847949\n",
       "19       1.432466\n",
       "20       0.823364\n",
       "21      -1.597949\n",
       "23      -0.468745\n",
       "24       0.095166\n",
       "25      -0.531215\n",
       "26       0.724560\n",
       "27      -0.034053\n",
       "28      -1.131461\n",
       "29       0.229053\n",
       "31       0.592230\n",
       "32       0.211439\n",
       "33      -0.067534\n",
       "34      -0.355553\n",
       "           ...   \n",
       "13546    1.555910\n",
       "13547   -3.028293\n",
       "13548    2.814781\n",
       "13551   -1.896107\n",
       "13553    0.193986\n",
       "13555   -1.004905\n",
       "13556   -0.074720\n",
       "13557   -0.789957\n",
       "13559    0.633041\n",
       "13560    0.574920\n",
       "13561   -0.446897\n",
       "13562    0.976070\n",
       "13563    0.489176\n",
       "13564    0.387405\n",
       "13565    0.941163\n",
       "13566    0.099560\n",
       "13569   -1.332066\n",
       "13570   -0.568945\n",
       "13571   -1.316138\n",
       "13572   -1.596667\n",
       "13573   -0.656245\n",
       "13574   -1.131445\n",
       "13576   -0.945486\n",
       "13578   -1.249289\n",
       "13579   -0.282626\n",
       "13581   -0.990532\n",
       "13584   -0.925080\n",
       "13588   -0.955495\n",
       "13591    0.292949\n",
       "13592    0.292949\n",
       "Length: 9038, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = results.resid\n",
    "y_hat = results.fittedvalues\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            totalwgt_lb   R-squared:                       0.005\n",
      "Model:                            OLS   Adj. R-squared:                  0.005\n",
      "Method:                 Least Squares   F-statistic:                     43.02\n",
      "Date:                Thu, 22 Feb 2018   Prob (F-statistic):           5.72e-11\n",
      "Time:                        19:44:42   Log-Likelihood:                -15897.\n",
      "No. Observations:                9038   AIC:                         3.180e+04\n",
      "Df Residuals:                    9036   BIC:                         3.181e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      6.8304      0.068    100.470      0.000       6.697       6.964\n",
      "agepreg        0.0175      0.003      6.559      0.000       0.012       0.023\n",
      "==============================================================================\n",
      "Omnibus:                     1024.052   Durbin-Watson:                   1.618\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3081.833\n",
      "Skew:                          -0.601   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.596   Cond. No.                         118.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that first babies tend to be lighter, we also saw that age has affect on birth weight. These could be related, but let's take a quick look before moving on to a **multiple regression** for the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12476118453549034"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_weight = firsts.totalwgt_lb.mean() - others.totalwgt_lb.mean()\n",
    "diff_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.586434766150152"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_age = firsts.agepreg.mean() - others.agepreg.mean()\n",
    "diff_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017453851471802843"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = smf.ols('totalwgt_lb ~ agepreg', data=live).fit()\n",
    "slope = results.params['agepreg']\n",
    "slope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the difference in age is about 3.5 years, the difference in weight is .124 pounds and we know each year adds 0.0175 pounds to the birth weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.062597099721694707"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slope * diff_age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this looks like about half of the difference being accounted for by the mother's age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "live['isfirst'] = live.birthord == 1 #add boolean indicator column\n",
    "formula = 'totalwgt_lb ~ isfirst'\n",
    "results = smf.ols(formula, data=live).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            totalwgt_lb   R-squared:                       0.002\n",
      "Model:                            OLS   Adj. R-squared:                  0.002\n",
      "Method:                 Least Squares   F-statistic:                     17.74\n",
      "Date:                Thu, 22 Feb 2018   Prob (F-statistic):           2.55e-05\n",
      "Time:                        19:44:42   Log-Likelihood:                -15909.\n",
      "No. Observations:                9038   AIC:                         3.182e+04\n",
      "Df Residuals:                    9036   BIC:                         3.184e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           7.3259      0.021    356.007      0.000       7.286       7.366\n",
      "isfirst[T.True]    -0.1248      0.030     -4.212      0.000      -0.183      -0.067\n",
      "==============================================================================\n",
      "Omnibus:                      988.919   Durbin-Watson:                   1.613\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2897.107\n",
      "Skew:                          -0.589   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.511   Cond. No.                         2.58\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ols** treats the isfirst column as a **categorical variable**. The estimated parameter on weight when isfirst is true is -.0125 pounds, which is the difference in birth weight between first babies and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slope and the intercept are statistically significant, which means that they were unlikely to occur by chance, but the the R2 value for this model is small, which means that isfirst doesn't account for a substantial part of the variation in birth weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'totalwgt_lb ~ agepreg'\n",
    "results = smf.ols(formula, data=live).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            totalwgt_lb   R-squared:                       0.005\n",
      "Model:                            OLS   Adj. R-squared:                  0.005\n",
      "Method:                 Least Squares   F-statistic:                     43.02\n",
      "Date:                Thu, 22 Feb 2018   Prob (F-statistic):           5.72e-11\n",
      "Time:                        19:44:43   Log-Likelihood:                -15897.\n",
      "No. Observations:                9038   AIC:                         3.180e+04\n",
      "Df Residuals:                    9036   BIC:                         3.181e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      6.8304      0.068    100.470      0.000       6.697       6.964\n",
      "agepreg        0.0175      0.003      6.559      0.000       0.012       0.023\n",
      "==============================================================================\n",
      "Omnibus:                     1024.052   Durbin-Watson:                   1.618\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3081.833\n",
      "Skew:                          -0.601   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.596   Cond. No.                         118.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agepreg results in a similary small $R^{2}$ with statistically significant parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'totalwgt_lb ~ isfirst + agepreg'\n",
    "results = smf.ols(formula, data=live).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            totalwgt_lb   R-squared:                       0.005\n",
      "Model:                            OLS   Adj. R-squared:                  0.005\n",
      "Method:                 Least Squares   F-statistic:                     24.02\n",
      "Date:                Thu, 22 Feb 2018   Prob (F-statistic):           3.95e-11\n",
      "Time:                        19:44:43   Log-Likelihood:                -15894.\n",
      "No. Observations:                9038   AIC:                         3.179e+04\n",
      "Df Residuals:                    9035   BIC:                         3.182e+04\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           6.9142      0.078     89.073      0.000       6.762       7.066\n",
      "isfirst[T.True]    -0.0698      0.031     -2.236      0.025      -0.131      -0.009\n",
      "agepreg             0.0154      0.003      5.499      0.000       0.010       0.021\n",
      "==============================================================================\n",
      "Omnibus:                     1019.945   Durbin-Watson:                   1.618\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3063.682\n",
      "Skew:                          -0.599   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.588   Cond. No.                         137.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we have noted the agepreg is nonlinear, so we might want to add a variable to caputre more of this relationship.\n",
    "\n",
    "A common choice is to add a variable that represents the squares of the ages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "live['agepreg2'] = live.agepreg**2\n",
    "formula = 'totalwgt_lb ~ isfirst + agepreg + agepreg2'\n",
    "results = smf.ols(formula, data=live).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            totalwgt_lb   R-squared:                       0.007\n",
      "Model:                            OLS   Adj. R-squared:                  0.007\n",
      "Method:                 Least Squares   F-statistic:                     22.64\n",
      "Date:                Thu, 22 Feb 2018   Prob (F-statistic):           1.35e-14\n",
      "Time:                        19:44:43   Log-Likelihood:                -15884.\n",
      "No. Observations:                9038   AIC:                         3.178e+04\n",
      "Df Residuals:                    9034   BIC:                         3.181e+04\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           5.6923      0.286     19.937      0.000       5.133       6.252\n",
      "isfirst[T.True]    -0.0504      0.031     -1.602      0.109      -0.112       0.011\n",
      "agepreg             0.1124      0.022      5.113      0.000       0.069       0.155\n",
      "agepreg2           -0.0018      0.000     -4.447      0.000      -0.003      -0.001\n",
      "==============================================================================\n",
      "Omnibus:                     1007.149   Durbin-Watson:                   1.616\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3003.343\n",
      "Skew:                          -0.594   Prob(JB):                         0.00\n",
      "Kurtosis:                       5.562   Cond. No.                     1.39e+04\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.39e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the coefficient of agepreg2 is negative, so this parabola curve downward. This model account for more variability in birth weight and results in a smaller parameter for isfirst, making it no longer statistically significant.\n",
    "\n",
    "We conclude that the apparent difference in birth weight is explained, at least in part, by the difference in mother's age. When we include mother's age in the model, the effect of isfirst gets smaller, and the remaining effect might be due to chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This process is still considered linear regression, because the dependent variable is a linear function of the explanatory variables, regardless of whether some variables are nonlinear functions of others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, mother's age acts as a **control variable**.\n",
    "\n",
    "When we include *agepreg* in the model it \"controls for\" the difference in age between first-time mothers and others, making it possible to isolate the effect (if any) of *isfirst*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using pandas to perform a SQL join operation\n",
    "\n",
    "live = live[live.prglngth>30]\n",
    "resp = nsfg.ReadFemResp()\n",
    "resp.index = resp.caseid #set index of respondents to the caseid\n",
    "join = live.join(resp, on='caseid', rsuffix='_r') #add suffix for duplicate cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for name in join.columns:\n",
    "    try:\n",
    "        if join[name].var() < 1e-7: #check that data has some variability\n",
    "            continue\n",
    "        #include agepreg in formula, we know it has predictive power\n",
    "        formula = 'totalwgt_lb ~ agepreg + ' + name\n",
    "        model = smf.ols(formula, data=join)\n",
    "        if model.nobs < len(join)/2: #check that number of observations \n",
    "            #is greater than half the length of a column.\n",
    "            continue\n",
    "        results = model.fit() #fit model\n",
    "        \n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "    t.append((results.rsquared, name)) #appended the r2 for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0053576473236406352, 'caseid'),\n",
       " (0.0057500139850770182, 'pregordr'),\n",
       " (0.0063309802373902047, 'pregend1'),\n",
       " (0.016017752709788113, 'nbrnaliv'),\n",
       " (0.005543156193094867, 'cmprgend')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalwgt_lb 1.0\n",
      "birthwgt_lb 0.949812730598\n",
      "lbw1 0.300824078447\n",
      "prglngth 0.130125194886\n",
      "wksgest 0.123400413634\n",
      "agecon 0.102031499282\n",
      "mosgest 0.0271442746396\n",
      "babysex 0.0185509252939\n",
      "race_r 0.0161995035863\n",
      "race 0.0161995035863\n",
      "nbrnaliv 0.0160177527098\n",
      "paydu 0.0140037955781\n",
      "rmarout03 0.0134300664657\n",
      "birthwgt_oz 0.0131024576157\n",
      "anynurse 0.0125290225418\n",
      "bfeedwks 0.0121936884045\n",
      "totincr 0.0118700690312\n",
      "marout03 0.0118078019944\n",
      "marcon03 0.0117525993544\n",
      "cebow 0.0114377709196\n",
      "rmarout01 0.0114077371386\n",
      "rmarout6 0.0113541384728\n",
      "marout01 0.0112693572468\n",
      "hisprace_r 0.011238349302\n",
      "hisprace 0.011238349302\n",
      "mar1diss 0.0109615635908\n",
      "fmarcon5 0.0106049646843\n",
      "rmarout02 0.0105469132066\n",
      "marcon02 0.0104814017955\n",
      "fmarout5 0.0104616913674\n"
     ]
    }
   ],
   "source": [
    "#sort and find highest R2 values\n",
    "t.sort(reverse=True)\n",
    "for mse, name in t[:30]: #print 30 highest R2 values\n",
    "    print(name, mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you start with a theory and use data to test it. Other times you start with data and go looking for possible theories. The second approach, which this section demonstrates, is called **data mining**. \n",
    "\n",
    "An advantage of data mining is that it can discover unexpected patterns. A hazard is that many of the patterns it discovers are either random or spurious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = ('totalwgt_lb ~ agepreg + C(race) + babysex==1 +' \n",
    "           'nbrnaliv>1 + paydu==1 + totincr')\n",
    "#C(race) says to treat the numerical data in the race column as categorical\n",
    "\n",
    "#encoding for babysex is 1 for male, 2 for female; writing babysex==1\n",
    "#converts it to boolean, True for male and false for female.\n",
    "\n",
    "#nbrnaliv>1 is True for multiple births\n",
    "#paydu==1 is True for respondents who own their houses.\n",
    "\n",
    "results = smf.ols(formula, data=join).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:            totalwgt_lb   R-squared:                       0.060\n",
      "Model:                            OLS   Adj. R-squared:                  0.059\n",
      "Method:                 Least Squares   F-statistic:                     79.98\n",
      "Date:                Thu, 22 Feb 2018   Prob (F-statistic):          4.86e-113\n",
      "Time:                        20:12:59   Log-Likelihood:                -14295.\n",
      "No. Observations:                8781   AIC:                         2.861e+04\n",
      "Df Residuals:                    8773   BIC:                         2.866e+04\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept                6.6303      0.065    102.223      0.000       6.503       6.757\n",
      "C(race)[T.2]             0.3570      0.032     11.215      0.000       0.295       0.419\n",
      "C(race)[T.3]             0.2665      0.051      5.175      0.000       0.166       0.367\n",
      "babysex == 1[T.True]     0.2952      0.026     11.216      0.000       0.244       0.347\n",
      "nbrnaliv > 1[T.True]    -1.3783      0.108    -12.771      0.000      -1.590      -1.167\n",
      "paydu == 1[T.True]       0.1196      0.031      3.861      0.000       0.059       0.180\n",
      "agepreg                  0.0074      0.003      2.921      0.004       0.002       0.012\n",
      "totincr                  0.0122      0.004      3.110      0.002       0.005       0.020\n",
      "==============================================================================\n",
      "Omnibus:                      398.813   Durbin-Watson:                   1.604\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1388.362\n",
      "Skew:                          -0.037   Prob(JB):                    3.32e-302\n",
      "Kurtosis:                       4.947   Cond. No.                         221.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can be generalized to handle other kinds of dependent variables. If the dependent variable is boolean, the generalized model is called **logistic regression**. If the dependent variable is an integer count, it's called **Poisson regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to use linear regression to predict a boolean, we might encode True as 1, False as 0. This produces predictions that are hard to predict, like $y=0.5$. It's tempting to read this as a probability, but it isn't, this model could also output $y=1.1$ or $y=-0.1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression avoids this problem by expressing predictions in terms of odds rather than probabilities. Odds of an event is the ratio of the probability it will occur to the probability\n",
    "that it will not.\n",
    "\n",
    "So if I think my team has a **75% chance** of winning, I would say that the odds in their favor are **three to one**, because the chance of winning is three times the chance of losing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easily convert between odds and probabilities;\n",
    "$$\\text{odds} = \\frac{p}{(1-p)}$$\n",
    "\n",
    "or;\n",
    "\n",
    "$$p = \\frac{\\text{odds}}{(\\text{odds}+1)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is based on the following model;\n",
    "\n",
    "$$log(o) = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\epsilon$$\n",
    "\n",
    "where $o$ is the odds in favor of a particular outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike linear regression, logistic regression does not have a closed form solution, so it is solved by guessing an initial solution and improving it  iteratively.\n",
    "\n",
    "The usual goal is to find the maximum-likelihood estimate (MLE), which is the set of parameters that maximizes the likelihood of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intial data\n",
    "y = np.array([0, 1, 0, 1])\n",
    "x1 = np.array([0, 0, 0, 1])\n",
    "x2 = np.array([0, 1, 1, 1])\n",
    "\n",
    "#starting guesses\n",
    "beta = [-1.5, 2.8, 1.1]\n",
    "\n",
    "#compute each row\n",
    "log_o = beta[0] + beta[1] * x1 + beta[2] * x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert log odds to odds\n",
    "o = np.exp(log_o)\n",
    "#convert odds to probabilities\n",
    "p = o / (o+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of an outcome is $p$ when $y=1$ and $1-p$ when $y=0$. For example, if we think the probability of a boy is 0.8 and the outcome is a boy, the likelihood is 0.8; if the outcome is a girl, the likelihood is 0.2. \n",
    "\n",
    "We can compute the likelihoods and then the over **likelihood** of the data is the product of the likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes = y * p + (1-y) * (1-p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18009335296730339"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "like = np.prod(likes)\n",
    "like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these values of *beta*, the likelihood of the data is 0.18. \n",
    "\n",
    "The goal of logistic regression is to find parameters that maximize this likelihood. To do that, most statistics packages use an iterative solver like Newton's method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StatsModels` provides an implementation of logistic regression called *logit*,\n",
    "named for the function that converts from probability to log odds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*logit* requires the dependent variable to be binary (rather than boolean), so we can use *astype(int)* to convert to binary integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "live['boy'] = (live.babysex==1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693015\n",
      "         Iterations 3\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                    boy   No. Observations:                 8884\n",
      "Model:                          Logit   Df Residuals:                     8882\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Thu, 22 Feb 2018   Pseudo R-squ.:               6.144e-06\n",
      "Time:                        20:30:30   Log-Likelihood:                -6156.7\n",
      "converged:                       True   LL-Null:                       -6156.8\n",
      "                                        LLR p-value:                    0.7833\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.0058      0.098      0.059      0.953      -0.185       0.197\n",
      "agepreg        0.0010      0.004      0.275      0.783      -0.006       0.009\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "model = smf.logit('boy ~ agepreg', data=live)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a Logit object that represents the model. It contains attributes called `endog` and `exog` that contain the **endogenous variable**, another name for the dependent variable, and the **exogenous variables**, another name for the explanatory variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "endog = pd.DataFrame(model.endog, columns=[model.endog_names])\n",
    "exog = pd.DataFrame(model.exog, columns=model.exog_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter of *agepreg* is positive, which suggests that older mothers are more likely to have boys, but the p-value is 0.783, which means that the apparent effect could easily be due to chance.\n",
    "\n",
    "The coefficient of determination, $R^{2}$, does not apply to logistic regression, but there are several alternatives that are used as \"pseudo $R^{2}$ values.\" These values can be useful for comparing models. \n",
    "\n",
    "For example, here's a model that includes several factors believed to be associated with sex ratio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692944\n",
      "         Iterations 3\n"
     ]
    }
   ],
   "source": [
    "formula = 'boy ~ agepreg + hpagelb + birthord + C(race)'\n",
    "model = smf.logit(formula, data=live)\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                    boy   No. Observations:                 8782\n",
      "Model:                          Logit   Df Residuals:                     8776\n",
      "Method:                           MLE   Df Model:                            5\n",
      "Date:                Thu, 22 Feb 2018   Pseudo R-squ.:               0.0001440\n",
      "Time:                        20:33:52   Log-Likelihood:                -6085.4\n",
      "converged:                       True   LL-Null:                       -6086.3\n",
      "                                        LLR p-value:                    0.8822\n",
      "================================================================================\n",
      "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept       -0.0301      0.104     -0.290      0.772      -0.234       0.173\n",
      "C(race)[T.2]    -0.0224      0.051     -0.439      0.660      -0.122       0.077\n",
      "C(race)[T.3]    -0.0005      0.083     -0.005      0.996      -0.163       0.162\n",
      "agepreg         -0.0027      0.006     -0.484      0.629      -0.014       0.008\n",
      "hpagelb          0.0047      0.004      1.112      0.266      -0.004       0.013\n",
      "birthord         0.0050      0.022      0.227      0.821      -0.038       0.048\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check our work, we compute the accuracy of our model.\n",
    "\n",
    "In the NSFG data, there are more boys than girls, so the baseline statregy is to guess boy each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual = endog['boy']\n",
    "baseline = actual.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = (results.predict() >= 0.5)\n",
    "true_pos = predict * actual\n",
    "true_neg = (1 - predict) * (1 - actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*results.predict* returns a NumPy array of probabilities, which we round off to 0 or 1. \n",
    "\n",
    "Multiplying by actual yields 1 if we predict a boy and get it right, 0 otherwise. \n",
    "\n",
    "So, true_pos indicates \"true positives\".\n",
    "\n",
    "Similarly, true_neg indicates the cases where we guess \"girl\" and get it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51150079708494645"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = (sum(true_pos) + sum(true_neg)) / len(actual)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is 0.512, slightly better than the baseline, 0.507. \n",
    "\n",
    "But, you should not take this result too seriously. We used the same data to build and test the model, so the model may not have predictive power on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boy\n"
     ]
    }
   ],
   "source": [
    "#predict sex of a friend who is 35, with a 39 year old husband\n",
    "#and its their third kid\n",
    "columns = ['agepreg', 'hpagelb', 'birthord', 'race']\n",
    "new = pd.DataFrame([[35, 39, 3, 2]], columns=columns)\n",
    "y = results.predict(new)\n",
    "print('boy') if y[0]>0.5 else print('girl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "**Exercise 11.1** Suppose one of your co-workers is expecting a baby and you are participating in an office pool to predict the date of birth. Assuming that bets are placed during the 30th week of pregnancy, what variables could you use to make the best prediction? You should limit yourself to variables that are known before the birth, and likely to be available to the people in the pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = []\n",
    "for name in join.columns:\n",
    "    try:\n",
    "        if join[name].var() < 1e-7: \n",
    "            continue\n",
    "            \n",
    "        formula = 'prglngth ~ ' + name\n",
    "        model = smf.ols(formula, data=join)\n",
    "        if model.nobs < len(join)/2: \n",
    "            continue\n",
    "        results = model.fit() #fit model\n",
    "        \n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "    t.append((results.rsquared, name)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prglngth 1.0\n",
      "wksgest 0.806243411614\n",
      "totalwgt_lb 0.124457431481\n",
      "birthwgt_lb 0.119773078049\n",
      "lbw1 0.103725422046\n",
      "mosgest 0.0956243198959\n",
      "canhaver 0.0060504952682\n",
      "nbrnaliv 0.00457756578553\n",
      "anynurse 0.00245202488371\n",
      "bfeedwks 0.00236918394467\n",
      "pregend1 0.0022493894338\n",
      "cmlastlb_r 0.0020431424422\n",
      "cmlastlb 0.0020431424422\n",
      "evuseint 0.00189175277586\n",
      "paydu 0.00187682190302\n",
      "anymschp 0.00177999843325\n",
      "gestasun_m 0.00165713195501\n",
      "hlpmc 0.00161252106165\n",
      "diabetes 0.001600707227\n",
      "ptsb4mar 0.00158691214383\n",
      "cmhivtst 0.00148078379171\n",
      "todayspg 0.00147583725852\n",
      "marbefhx 0.00144127783259\n",
      "widrawal 0.00141412298374\n",
      "liv1chld 0.00141084482997\n",
      "methdiss 0.00140159126169\n",
      "sest_r 0.00132236819817\n",
      "sest 0.00132236819817\n",
      "manrel 0.00131095845942\n",
      "matchfound 0.00130910737713\n"
     ]
    }
   ],
   "source": [
    "t.sort(reverse=True)\n",
    "t = [col for col in t if col[1][-1] != 'i'] #remove indicator cols\n",
    "for mse, name in t[:30]: #print 30 highest R2 values\n",
    "    print(name, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nbrnaliv number of live births\n",
    "#paydu rent or own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = ('prglngth ~ nbrnaliv>1 + paydu==1')\n",
    "\n",
    "#nbrnaliv>1 is True for multiple births\n",
    "#paydu==1 is True for respondents who own their houses.\n",
    "\n",
    "results = smf.ols(formula, data=join).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               prglngth   R-squared:                       0.010\n",
      "Model:                            OLS   Adj. R-squared:                  0.010\n",
      "Method:                 Least Squares   F-statistic:                     46.65\n",
      "Date:                Thu, 22 Feb 2018   Prob (F-statistic):           6.98e-21\n",
      "Time:                        21:07:10   Log-Likelihood:                -18252.\n",
      "No. Observations:                8884   AIC:                         3.651e+04\n",
      "Df Residuals:                    8881   BIC:                         3.653e+04\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "========================================================================================\n",
      "                           coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------\n",
      "Intercept               38.8430      0.028   1375.690      0.000      38.788      38.898\n",
      "nbrnaliv > 1[T.True]    -1.5093      0.164     -9.182      0.000      -1.832      -1.187\n",
      "paydu == 1[T.True]       0.1175      0.040      2.933      0.003       0.039       0.196\n",
      "==============================================================================\n",
      "Omnibus:                     1556.606   Durbin-Watson:                   1.620\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6047.201\n",
      "Skew:                          -0.834   Prob(JB):                         0.00\n",
      "Kurtosis:                       6.681   Cond. No.                         9.36\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11.2** The Trivers-Willard hypothesis suggests that for many mammals the sex ratio depends on \"maternal condition\"; that is, factors like the\n",
    "mother's age, size, health, and social status. \n",
    "\n",
    "Some studies have shown this effect among humans, but results are mixed.\n",
    "\n",
    "In this chapter we tested some variables related to these factors, but didn't find any with a statistically significant effect on sex ratio.\n",
    "\n",
    "As an exercise, use a data mining approach to test the other variables in the pregnancy and respondent files. Can you find any factors with a substantial\n",
    "effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692996\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692962\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692850\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693001\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692903\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692766\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693019\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692995\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693017\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692989\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692579\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.686617\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693026\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692903\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693033\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692978\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692965\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692915\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693104\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693112\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692594\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693011\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693018\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693019\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692778\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692843\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692628\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693006\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692825\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692949\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693029\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692976\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693036\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692678\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692612\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693047\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693079\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692923\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692921\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693004\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692910\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693015\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692908\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693016\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692547\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692656\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692590\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692808\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692293\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692935\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693005\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692942\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693006\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692945\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692875\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692879\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692879\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692846\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692949\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692989\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692987\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693013\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692991\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693011\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692998\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692876\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692999\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692988\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693002\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692960\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693015\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692929\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692992\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692575\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692990\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692820\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692591\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692880\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692655\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692994\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692922\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692987\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692955\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693013\n",
      "         Iterations 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692895\n",
      "         Iterations 5\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.692784\n",
      "         Iterations: 35\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692643\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693019\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692840\n",
      "         Iterations 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692975\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692989\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692975\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693007\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693002\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693019\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692978\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692977\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692815\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693018\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693019\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693016\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693008\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692999\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.692985\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693018\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693007\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.686323\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693007\n",
      "         Iterations 3\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.693011\n",
      "         Iterations 3\n"
     ]
    }
   ],
   "source": [
    "t = []\n",
    "for name in join.columns:\n",
    "    try:\n",
    "        if join[name].var() < 1e-7: \n",
    "            continue\n",
    "            \n",
    "        formula = 'boy ~ ' + name\n",
    "        try:\n",
    "            model = smf.logit(formula, data=live)\n",
    "            nobs = len(model.endog)\n",
    "            if nobs < len(live)/2:\n",
    "                continue\n",
    "            results = model.fit() #fit model\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    except (ValueError, TypeError):\n",
    "        continue\n",
    "    t.append((results.prsquared, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totalwgt_lb 0.00967200535184\n",
      "birthwgt_lb 0.0092489816712\n",
      "lbw1 0.00104859533196\n",
      "frsteatd 0.0007600842059\n",
      "fmarout5 0.000681996122597\n",
      "brnout 0.000641068785277\n",
      "bpa_bdscheck1 0.000635132392883\n",
      "rmarout6 0.000620359718917\n",
      "pmarpreg 0.000525037533603\n",
      "cmprgbeg 0.000385150531009\n",
      "cmintstr 0.000347849734469\n",
      "fmarcon5 0.000305032394269\n",
      "cmintfin 0.000254556542944\n",
      "fmarital 0.000250894216646\n",
      "pregend1 0.000244550097046\n",
      "evuseint 0.000226956417488\n",
      "cmbirth 0.000208672062879\n",
      "pregnum 0.000207028475367\n",
      "agescrn 0.000202205053819\n",
      "ager 0.000202205053819\n",
      "wantbold 0.000199452287779\n",
      "cmprgend 0.000167794737359\n",
      "cmbabdob 0.000167794737359\n",
      "datecon 0.000160855399429\n",
      "datend 0.000158255290499\n",
      "prglngth 0.000141756277198\n",
      "anyusint 0.000139420326087\n",
      "religion 0.000130989904309\n",
      "oldwantp 0.000112175099662\n",
      "wantpart 0.000107334273798\n"
     ]
    }
   ],
   "source": [
    "t.sort(reverse=True)\n",
    "t = [col for col in t if col[1][-1] != 'i'] #remove indicator cols\n",
    "for mse, name in t[:30]: #print 30 highest R2 values\n",
    "    print(name, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "PatsyError",
     "evalue": "Error evaluating factor: NameError: name 'infever' is not defined\n    boy ~ agepreg + fmarout5==5 + infever==1\n                                  ^^^^^^^^^^",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, expr, source_name, inner_namespace)\u001b[0m\n\u001b[1;32m    165\u001b[0m         return eval(code, {}, VarLookupDict([inner_namespace]\n\u001b[0;32m--> 166\u001b[0;31m                                             + self._namespaces))\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'infever' is not defined",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mPatsyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-6e963dfabd3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mformula\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'boy ~ agepreg + fmarout5==5 + infever==1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformula\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/statsmodels/base/model.py\u001b[0m in \u001b[0;36mfrom_formula\u001b[0;34m(cls, formula, data, subset, drop_cols, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         tmp = handle_formula_data(data, None, formula, depth=eval_env,\n\u001b[0;32m--> 155\u001b[0;31m                                   missing=missing)\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesign_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/statsmodels/formula/formulatools.py\u001b[0m in \u001b[0;36mhandle_formula_data\u001b[0;34m(Y, X, formula, depth, missing)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_using_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n\u001b[0;32m---> 65\u001b[0;31m                                NA_action=na_action)\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             result = dmatrices(formula, Y, depth, return_type='dataframe',\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36mdmatrices\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0meval_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalEnvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     (lhs, rhs) = _do_highlevel_design(formula_like, data, eval_env,\n\u001b[0;32m--> 310\u001b[0;31m                                       NA_action, return_type)\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlhs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mPatsyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model is missing required outcome variables\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_do_highlevel_design\u001b[0;34m(formula_like, data, eval_env, NA_action, return_type)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     design_infos = _try_incr_builders(formula_like, data_iter_maker, eval_env,\n\u001b[0;32m--> 165\u001b[0;31m                                       NA_action)\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdesign_infos\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         return build_design_matrices(design_infos, data,\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/highlevel.py\u001b[0m in \u001b[0;36m_try_incr_builders\u001b[0;34m(formula_like, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m     68\u001b[0m                                       \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                       \u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                                       NA_action)\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/build.py\u001b[0m in \u001b[0;36mdesign_matrix_builders\u001b[0;34m(termlists, data_iter_maker, eval_env, NA_action)\u001b[0m\n\u001b[1;32m    694\u001b[0m                                                    \u001b[0mfactor_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                                                    \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                                                    NA_action)\n\u001b[0m\u001b[1;32m    697\u001b[0m     \u001b[0;31m# Now we need the factor infos, which encapsulate the knowledge of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;31m# how to turn any given factor into a chunk of data:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/build.py\u001b[0m in \u001b[0;36m_examine_factor_types\u001b[0;34m(factors, factor_states, data_iter_maker, NA_action)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_iter_maker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamine_needed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mguess_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfactor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcat_sniffers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/eval.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, memorize_state, data)\u001b[0m\n\u001b[1;32m    564\u001b[0m         return self._eval(memorize_state[\"eval_code\"],\n\u001b[1;32m    565\u001b[0m                           \u001b[0mmemorize_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                           data)\n\u001b[0m\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0m__getstate__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_pickling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/eval.py\u001b[0m in \u001b[0;36m_eval\u001b[0;34m(self, code, memorize_state, data)\u001b[0m\n\u001b[1;32m    549\u001b[0m                                  \u001b[0mmemorize_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_env\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m                                  \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                                  inner_namespace=inner_namespace)\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmemorize_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/compat.py\u001b[0m in \u001b[0;36mcall_and_wrap_exc\u001b[0;34m(msg, origin, f, *args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m                                  origin)\n\u001b[1;32m    123\u001b[0m             \u001b[0;31m# Use 'exec' to hide this syntax from the Python 2 parser:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raise new_exc from e\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# In python 2, we just let the original exception escape -- better\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3_501/lib/python3.6/site-packages/patsy/compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mPatsyError\u001b[0m: Error evaluating factor: NameError: name 'infever' is not defined\n    boy ~ agepreg + fmarout5==5 + infever==1\n                                  ^^^^^^^^^^"
     ]
    }
   ],
   "source": [
    "formula='boy ~ agepreg + fmarout5==5 + infever==1'\n",
    "model = smf.logit(formula, data=live)\n",
    "results = model.fit()\n",
    "results.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
